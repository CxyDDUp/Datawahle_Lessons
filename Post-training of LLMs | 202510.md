# 第一章 课程介绍

## 1.1 课程介绍

### 1.1.1 大模型训练的概述

+ 两个阶段：

  + 预训练阶段：

    + 模型学习预测下一个词或者标记。

    + > ==我认为==，这是后训练阶段的基石模型，通常是由各大厂商或者学术机构研究出来的基础模型。由于其庞大的数据规模，需要很大的资源成本，所以这并非是我们平时涉及的部分，只需要学会选取合适领域的基座模型即可。
      >
      > <font color=red>但值得留意得是：</font>一些常见大规模模型提出的结构及方法，需要了解及深思，例如Transformer架构、多头潜在注意力（MLA）、混合专家（MoE）等

  + 后训练阶段：

    + 模型通过进一步训练以执行更具体的任务。

    + > ==我认为==，此阶段应该就是平时的横向课题任务，所涉及的对大模型应用开发的垂直落地，由于垂直领域数据集的优先呢

### 1.1.2 后训练方法概述

+ 监督微调（SFT）：
  + 带<font color=red>标注</font>的提示🔔-响应对应的训练模型
  + ==核心==：让模型模仿输入提示与输出响应间的映射关系
  + 适用范围：引入**新**行为或对模型进行**重大调整**
+ 直接偏好优化（DPO）：
  + 向模型展示同一提示下的“优质”和“劣质”答案，驱动模型学习（<font color=red>？？</font>Maybe类似于强化学习的奖励机制❌）
    + `DPO`通过构造性损失函数，让模型趋近优质响应而远离劣质响应。
    + Example：
      + “我是你的助手”：劣质响应
      + “我是您的AI助手”：优质响应
      + 使用`DPO`调整一个`Qwen`指令模型的“身份🆔认知”
+ 在线强化学习（Online RL）：
  + 让模型接收提示并==生成响应==，根据奖励函数对响应质量进行**评分**，模型再根据评分结果进行更新。
  + 如何获取奖励函数？？？
    + 人工对响应质量进行评判，即训练出一个与人类判断水准一致的评分函数。
      + 例如：近端策略优化
    + 可验证奖励
      + 适用范围：数学或编程等具有客观正确性标准的任务
      + 例如：数学验证器或单元测试来判定解题步骤/代码是否正确
  + Summry：
    + 利用正确性来作为奖励函数
    + 例子🌰：DeepSeek团队提出的`GRPO`算法

### 1.1.3 总结

1. 大模型训练一般是分为俩steps：预训练阶段（一般不考虑，直接调用）+后训练阶段（重点学习）
2. 常见的后训练技术又分为三种：SFT、DPO、Online RL
3. 记录一下关于DPO与Online RL此刻感受的区别所在

|         方面         |                             DPO                              |                          Online RL                           |
| :------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
|       核心思想       | 将偏好数据（==人类标注==的优劣回答组合）直接建模为一个分类问题，通过最大偏好回答的隐式奖励来优化模型 | 使用强化学习框架：模型生成回答，奖励模型(RM)给出分数，通过策略梯度更新模型。 |
| 是否需要显式奖励模型 | ❌<br />因为DPO通过数学变换的方式，将奖励函数“隐式”地嵌入到损失函数中 |          ✅<br />必须先训练一个独立的奖励模型来打分           |
|   是否需要参考模型   |       ✅<br />用于计算KL散度，防止模型偏离原始行为太多        |                         ✅<br />同左                          |

<font color=red>Tips:</font>

> KL散度：即相对熵。
>
> + 衡量两个概率分布之间差异的重要工具
> + KL散度越大，则两个真实分布所差就越远！！！

## 1.2 后训练技术介绍

### 1.2.1 后训练

+ 从随机初始化的模型开始预训练
  + 从各类数据源学习知识，==通常数据量非常庞大==
  + **Result:** Get一个基础模型`base`
+ 从`base`模型开始，进行后训练
  + Target: 从精心筛选的数据中学习到响应模式。==垂直领域的应用==
  + **Result:** Update为一个指令模型或者对话模型
+ 还可以进一步进行后训练
  + Target: 调整模型行为或者增强特定能力。==垂直领域的细节应用==

### 1.2.2 预训练

+ 通常被视作：<font color=red>无监督学习</font>
+ 数据集特点：
  + 大规模
  + 无标注
  + 文本语料
+ 概率公式：

$p(x_{1:L})=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)....p(x_L|x_{1:L-1})=\prod_{i=1}^{L}p(x_i|x_{1:L-1})$

+ 目的：使得模型被训练成能根据已经给出的标记==预测==下一个标记

### 1.2.3 后训练技术

+ 监督微调（SFT）：
  + 监督学习，即带标签的数据集
  + 提示：给模型的指令；响应：模型应该给出的理想回答
  + 此过程需要1000至10亿个标记，远少于预训练规模
  + <font color=red>训练损失关键区别:</font>仅对响应标记进行训练，而不涉及提示标记
+ 直接偏好优化（DPO）：
  + 创建包含提示及其对应**优质/劣质**响应的数据集。
  + 特点：针对任意一个提示🔔，可生成多个响应并筛选出优质与劣质样本
  + 目的：使得模型远离劣质响应并且学习优质响应
  + 此过程需要1000至10亿个标记，远少于预训练规模
+ 在线强化学习（Online RL）：
  + 只需提示集+奖励函数
  + 提示———>语言模型———>生成响应———>通过奖励函数对该响应进行评分———>利用评分更新模型
  + 需要1000至1000万个提示
  + 目的：通过模型自身生成的响应来最大化奖励值
+ 后训练的三个关键因素：
  + ==数据与算法的协同设计==：选择适宜的数据结构以及良好的协同设计
  + ==可靠的高效的算法库==：`HuggingFace TRL`+`Open RLLHF`+`veRL`+`Nemo RL`
  + ==合理的评估体系==：
    + 替代人类评判的`LLM`评估：`AlpacaEval` \ `MT Bench` \ `Arena Hard`
      + `AlpacaEval`: 给出一个指令让模型A和B回答，再由LLM判断胜负
      + `MT Bench`：(Multi-Turn Benchmark) 使用``LLM`生成80个复杂的多轮对话，模型需要连续回答每一轮。LLM对其进行打分
      + `Arena Hard`：从用户真实提交的“最难问题”中筛选出一批最具挑战性的指令，让``LLM`作为裁判
    + 指令模型静态基准：`LiveCodeBench`(热门代码基准) \ `AIME 2024/2025`
      + `LiveCodeBench`:：模型生成代码->在真实测试用例上运行->计算通过率
      + `AIME 2024/2025`：模型生成解题过程->判断最终答案是否正确
    + 知识与推理数据集：`GPQA` \ `MMLU Pro`
      + `GPQA`：评估模型在博士级别专业知识上的表现
      + `MMLU Pro`：测试跨学科、多任务理解能力
    + 指令遵循评估: `IFEval`
      + `IFEval`：对细粒度指令对遵循程度
    + 函数调用与智能体评估：`BFCL` \ `NexusBench` \ `TauBench` \ `ToolSandbox`
      + `BFCL`：给定用户请求，判断是否需要调用函数
      + `NexusBench`：多步骤任务的表现
      + `TauBench`：多工具协同和动态决策
      + `ToolSandbox`：用于测试模型调用真实工具的能力
    + <font color=red>值得注意⚠️的是</font>：
      + 提升单一指标相对容易
      + 重要前提：不损害其他领域的能力

# 第二章 监督微调理论及实践

# 第三章 直接偏好优化理论及实践

# 第四章 在线强化学习理论及实践

# 第五章 课程总结